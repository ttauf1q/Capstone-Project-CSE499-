# -*- coding: utf-8 -*-
"""Llama3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fJR69TQIO7apoPda-NF7js6WXeymSYi6
"""

!pip install transformers torch accelerate

from google.colab import userdata
my_secret_key = userdata.get('HF_TOKEN')

!huggingface-cli login --token "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"

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import textwrap

# Define model and tokenizer
model_id = "meta-llama/Meta-Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

# Set up the pipeline with the model, tokenizer, and padding token ID
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

# Define the prompt and generate the output with optimized parameters
prompt = "who is better Ronaldo or Messi?"
outputs = pipe(
    prompt,
    max_new_tokens=50,  # Set to a lower number for faster generation
    temperature=0.7,    # Lower temperature
    top_k=10,           # Control token randomness
    pad_token_id=tokenizer.eos_token_id
)

# Display the formatted output
response_content = outputs[0]["generated_text"]
wrap_width = 70
print("User:", prompt)
print("\nAssistant:\n")
for line in response_content.split("\n"):
    print(textwrap.fill(line, width=wrap_width))

