# -*- coding: utf-8 -*-
"""Llama3.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H2q9tcVBDMUwvWDhnPo7Y8-f7581_7IL
"""

!pip install transformers torch accelerate bitsandbytes

import os

# Replace 'your_hugging_face_token_here' with your actual token
os.environ['HF_TOKEN'] = 'your_hugging_face_token_here'

from google.colab import userdata
my_secret_key = userdata.get('HF_TOKEN')

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

!huggingface-cli login --token "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"

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
import torch
import textwrap

# Define model and tokenizer
model_id = "meta-llama/Llama-3.2-3B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")

# Set up the pipeline with the model, tokenizer, and padding token ID
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

# Define the prompt and generate the output with optimized parameters
prompt = "Find a word in Oceans contain sharks"
outputs = pipe(
    prompt,
    max_new_tokens=50,  # Set to a lower number for faster generation
    temperature=0.7,    # Lower temperature
    top_k=10,           # Control token randomness
    pad_token_id=tokenizer.eos_token_id
)

# Display the formatted output
response_content = outputs[0]["generated_text"]
wrap_width = 70
print("User:", prompt)
print("\nAssistant:\n")
for line in response_content.split("\n"):
    print(textwrap.fill(line, width=wrap_width))

